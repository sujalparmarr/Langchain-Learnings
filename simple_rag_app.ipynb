{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Simple RAG App with LangChain\n", "This notebook demonstrates a very simple Retrieval-Augmented Generation (RAG) app using LangChain. It is written for absolute beginners."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install langchain openai faiss-cpu"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n", "from langchain.text_splitter import CharacterTextSplitter\n", "from langchain.vectorstores import FAISS\n", "from langchain.chains import RetrievalQA\n", "\n", "# Load some example text\n", "text = \"\"\"\n", "LangChain is a framework for building applications powered by language models.\n", "It helps with chaining together components like LLMs, retrievers, and tools.\n", "RAG stands for Retrieval-Augmented Generation, where we fetch documents and use them to answer queries.\n", "\"\"\"\n", "\n", "# Split text into chunks\n", "splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n", "docs = splitter.create_documents([text])\n", "\n", "# Create embeddings and store in FAISS\n", "embeddings = OpenAIEmbeddings()\n", "vectorstore = FAISS.from_documents(docs, embeddings)\n", "\n", "# Create retriever\n", "retriever = vectorstore.as_retriever()\n", "\n", "# Create a simple QA chain\n", "qa = RetrievalQA.from_chain_type(\n", "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n", "    retriever=retriever\n", ")\n", "\n", "# Ask a question\n", "query = \"What is LangChain?\"\n", "result = qa.run(query)\n", "print(result)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}